"""
models/law_model.py ‚Äì CRUD + Vector search cho b·∫£ng law_documents
"""

from __future__ import annotations
from typing import Any
from models.db import get_connection


def insert_chunk(chunk: dict[str, Any]) -> None:
    """
    Insert m·ªôt chunk lu·∫≠t v√†o b·∫£ng law_documents.

    Args:
        chunk: Dict v·ªõi c√°c key t∆∞∆°ng ·ª©ng c·ªôt trong b·∫£ng.
    """
    
    sql = """
        INSERT INTO law_documents (
            law_name, 
            law_code, 
            document_type, 
            issuing_body, 
            field,
            chapter, 
            section, 
            article, 
            article_name, 
            clause, 
            point,
            content,
            chunk_id, 
            chunk_index, 
            embedding
        ) VALUES (
            %(law_name)s, 
            %(law_code)s, 
            %(document_type)s, 
            %(issuing_body)s, 
            %(field)s,
            %(chapter)s, 
            %(section)s, 
            %(article)s, 
            %(article_name)s, 
            %(clause)s, 
            %(point)s,
            %(content)s,
            %(chunk_id)s, 
            %(chunk_index)s, 
            %(embedding)s
        );
    """
    conn = get_connection()

    try:
        cur = conn.cursor()
        try:
            cur.execute(sql, chunk)
            conn.commit()
        except Exception as e:
            conn.rollback()
            # Xem SQL th·ª±c t·∫ø (mogrify)
            full_sql = cur.mogrify(sql, chunk).decode('utf-8')
            print(f"\n‚ùå L·ªñI INSERT: {e}")
            print(f"--- SQL: {full_sql[:500]}...")
            raise e
        finally:
            cur.close()
    except Exception as e:
        print(f"Error inserting chunk: {e}")
    finally:
        conn.close()


def vector_search(
    query_embedding: list[float],
    top_k: int = 5,
) -> list[dict[str, Any]]:
    """
    T√¨m ki·∫øm top-K chunks g·∫ßn nh·∫•t b·∫±ng cosine similarity.

    Args:
        query_embedding: Vector c√¢u h·ªèi (1536-dim).
        field: Lƒ©nh v·ª±c lu·∫≠t c·∫ßn l·ªçc (None = t·∫•t c·∫£).
        top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ.

    Returns:
        Danh s√°ch dict ch·ª©a th√¥ng tin t·ª´ng chunk.
    """
    params: list[Any] = [str(query_embedding), str(query_embedding), top_k]

    sql = f"""
        SELECT
            id, 
            law_name, law_code, document_type, issuing_body, field,
            chapter, article, article_name, clause, point, content,
            1 - (embedding <=> %s::vector) AS similarity
        FROM law_documents
        WHERE embedding IS NOT NULL
        ORDER BY embedding <=> %s::vector
        LIMIT %s;
    """

    conn = get_connection()
    try:
        cur = conn.cursor()
        cur.execute(sql, params)
        cols = [desc[0] for desc in cur.description]
        rows = [dict(zip(cols, row)) for row in cur.fetchall()]
        cur.close()
        print(f"‚úÖ T√¨m th·∫•y {len(rows)} k·∫øt qu·∫£.")
        import json
        with open("result.json", "w", encoding="utf-8") as f:
            json.dump(rows, f, indent=4, ensure_ascii=False)
        return rows
    finally:
        conn.close()


def count_records() -> int:
    """ƒê·∫øm t·ªïng s·ªë b·∫£n ghi trong b·∫£ng law_documents."""
    conn = get_connection()
    try:
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM law_documents;")
        result = cur.fetchone()
        cur.close()
        return result[0] if result else 0
    finally:
        conn.close()


def keyword_search(
    articles: list[str] | None = None,
    chapters: list[str] | None = None,
) -> list[dict[str, Any]]:
    """
    T√¨m ki·∫øm chunk ch√≠nh x√°c theo ƒêi·ªÅu/Ch∆∞∆°ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong c√¢u h·ªèi.
    C√°c chunk t√¨m ƒë∆∞·ª£c s·∫Ω c√≥ similarity = 1.0 (∆∞u ti√™n t·ªëi ƒëa).

    Args:
        articles: Danh s√°ch s·ªë ƒëi·ªÅu c·∫ßn t√¨m, v√≠ d·ª• ["2", "185"].
        chapters: Danh s√°ch s·ªë/t√™n ch∆∞∆°ng c·∫ßn t√¨m, v√≠ d·ª• ["I", "2"].

    Returns:
        Danh s√°ch chunk kh·ªõp, v·ªõi similarity = 1.0.
    """
    if not articles and not chapters:
        return []

    conditions: list[str] = []
    params: list[Any] = []

    if articles:
        placeholders = ", ".join(["%s"] * len(articles))
        conditions.append(f"article::text IN ({placeholders})")
        params.extend(articles)

    if chapters:
        or_parts = []
        for ch in chapters:
            or_parts.append("chapter ILIKE %s")
            params.append(f"%{ch}%")
        conditions.append(f"({' OR '.join(or_parts)})")

    where_clause = " OR ".join(conditions)
    sql = f"""
        SELECT
            id,
            law_name, law_code, document_type, issuing_body, field,
            chapter, article, article_name, clause, point, content,
            1.0::float AS similarity
        FROM law_documents
        WHERE {where_clause}
        ORDER BY article, clause;
    """

    conn = get_connection()
    try:
        cur = conn.cursor()
        cur.execute(sql, params)
        cols = [desc[0] for desc in cur.description]
        rows = [dict(zip(cols, row)) for row in cur.fetchall()]
        cur.close()
        print(f"üîë Keyword search: t√¨m th·∫•y {len(rows)} chunk kh·ªõp ch√≠nh x√°c.")
        return rows
    finally:
        conn.close()
