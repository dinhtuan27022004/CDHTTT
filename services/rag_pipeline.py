"""
services/rag_pipeline.py ‚Äì LangChain LCEL RAG pipeline
Pipeline: embed ‚Üí (keyword search + vector search) ‚Üí merge ‚Üí rerank ‚Üí build context ‚Üí LLM ‚Üí answer
"""

from __future__ import annotations
import re
from typing import Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

from models.embedding import get_embedding
from models.law_model import vector_search, keyword_search
from services.prompt_builder import RAG_PROMPT, build_context, format_citations
from services.openrouter_service import get_llm
import os
import json
from concurrent.futures import ThreadPoolExecutor
from services.reranker import rerank
from services.query_expansion import expand_query_for_search
from config.rag_config import SIM_THRESHOLD, RERANK_THRESHOLD, MAX_CANDIDATES_FETCH



def extract_legal_references(question: str) -> dict[str, list[str]]:
    """
    Ph√°t hi·ªán c√°c tham chi·∫øu Ch∆∞∆°ng/ƒêi·ªÅu c·ª• th·ªÉ trong c√¢u h·ªèi b·∫±ng regex.

    V√≠ d·ª•:
        "ƒêi·ªÅu 2 v√† ƒêi·ªÅu 185 quy ƒë·ªãnh g√¨?"
        ‚Üí {"articles": ["2", "185"], "chapters": []}

    Returns:
        Dict v·ªõi keys: articles, chapters (list[str]).
    """
    # ƒêi·ªÅu X
    articles = re.findall(r"[ƒëd]i[e·ªÅ·ªá·ªÉ·∫π·∫ª]u\s+?(\d+)", question, re.IGNORECASE)

    # Ch∆∞∆°ng X
    chapters = re.findall(r"ch[∆∞∆∞∆°]ng\s+([\dIVXLivxl]+)", question, re.IGNORECASE)

    return {
        "articles": list(dict.fromkeys(articles)),   # deduplicate, gi·ªØ th·ª© t·ª±
        "chapters": list(dict.fromkeys(chapters)),
    }


def _build_chain():
    """
    X√¢y d·ª±ng LCEL chain cho RAG.

    Lu·ªìng:
        {"context": ..., "question": ...}
        ‚Üí RAG_PROMPT
        ‚Üí ChatOpenAI (OpenRouter)
        ‚Üí StrOutputParser
    """
    llm = get_llm()
    chain = RAG_PROMPT | llm | StrOutputParser()
    return chain

def run_rag(
    question: str,
) -> dict[str, Any]:
    """
    Ch·∫°y to√†n b·ªô RAG pipeline t·ª± ƒë·ªông d·ª±a tr√™n ng∆∞·ª°ng ƒëi·ªÉm s·ªë (Threshold-based).

    Quy tr√¨nh:
    1. Embed c√¢u h·ªèi.
    2. Keyword search: ph√°t hi·ªán Ch∆∞∆°ng/ƒêi·ªÅu c·ª• th·ªÉ ‚Üí fetch DB (sim = 1.0).
    3. Vector search: l·∫•y k·∫øt qu·∫£ theo ng∆∞·ª°ng SIM_THRESHOLD.
    4. Merge + deduplicate.
    5. Rerank: ch·∫•m l·∫°i to√†n b·ªô ·ª©ng vi√™n b·∫±ng search_query.
    6. Build context ‚Üí Invoke LLM ‚Üí Tr·∫£ v·ªÅ k·∫øt qu·∫£.
    """

    # 1. Ch·∫°y song song: Query Expansion v√† Tr√≠ch xu·∫•t tham chi·∫øu lu·∫≠t
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_expansion = executor.submit(expand_query_for_search, question)
        future_refs = executor.submit(extract_legal_references, question)
        
        search_query = future_expansion.result()
        refs = future_refs.result()
        
    print(f"üöÄ Expanded Query: {search_query}")
    
    # 2. Sinh song song ti·∫øp: Keyword search v√† Embedding
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_kw = executor.submit(
            keyword_search, 
            articles=refs["articles"] or None, 
            chapters=refs["chapters"] or None
        )
        future_vec = executor.submit(get_embedding, search_query)
        
        kw_hits = future_kw.result()
        query_vec = future_vec.result()

    # 3. Vector search s∆° b·ªô (l·ªçc theo ng∆∞·ª°ng tr·ª±c ti·∫øp trong DB)
    vec_results = vector_search(query_vec, top_k=MAX_CANDIDATES_FETCH, threshold=SIM_THRESHOLD)

    # 4. Merge + deduplicate (Keyword hits lu√¥n ƒë∆∞·ª£c gi·ªØ v√† ƒë·ª©ng tr∆∞·ªõc)
    seen_ids: set = set()
    candidates: list[dict] = []
    
    # G·ªôp k·∫øt qu·∫£
    chunks = kw_hits + vec_results
    for chunk in chunks:
        cid = chunk.get("id")
        if cid not in seen_ids:
            seen_ids.add(cid)
            candidates.append(chunk)

    # [DEBUG] L∆∞u candidates ra file JSON
    try:
        with open("debug_candidates.json", "w", encoding="utf-8") as f:
            json.dump(candidates, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save debug_candidates.json: {e}")

    if not candidates:
        return {
            "answer":       f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu lu·∫≠t n√†o ƒë·ªß ƒë·ªô tin c·∫≠y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y (Similarity < {SIM_THRESHOLD}).",
            "citations":    [],
            "chunks":       [],
            "candidates":   [],
            "search_query": search_query,
        }

    # 6. Rerank: l·ªçc theo ng∆∞·ª°ng rerank_score >= 0.7
    chunks = rerank(search_query, candidates, score_threshold=RERANK_THRESHOLD)

    # [DEBUG] L∆∞u k·∫øt qu·∫£ sau Rerank ra file JSON
    try:
        with open("debug_results.json", "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save debug_results.json: {e}")

    if not chunks:
        return {
            "answer":       f"T√¨m th·∫•y t√†i li·ªáu li√™n quan nh∆∞ng ƒë·ªô ch√≠nh x√°c kh√¥ng ƒë·ªß cao (Rerank < {RERANK_THRESHOLD}) ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi.",
            "citations":    [],
            "chunks":       [],
            "candidates":   candidates,
            "search_query": search_query,
        }

    # 7. Build context
    context = build_context(chunks)

    # 8. Tr·∫£ v·ªÅ k·∫øt qu·∫£ (ƒê√£ g·ª° b·ªè Streaming)
    citations = format_citations(chunks)
    
    # Chu·∫©n b·ªã chain
    llm = get_llm()
    chain = RAG_PROMPT | llm | StrOutputParser()

    # [DEBUG] L∆∞u Prompt ƒë·∫ßy ƒë·ªß ra file TXT
    try:
        full_prompt_text = RAG_PROMPT.format(context=context, question=question)
        with open("debug_prompt.txt", "w", encoding="utf-8") as f:
            f.write(full_prompt_text)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save debug_prompt.txt: {e}")

    answer = chain.invoke({"context": context, "question": question})
    
    return {
        "answer":       answer,
        "citations":    citations,
        "chunks":       chunks,
        "candidates":   candidates,
        "search_query": search_query,
    }
