"""
services/rag_pipeline.py â€“ LangChain LCEL RAG pipeline
Pipeline: embed â†’ (keyword search + vector search) â†’ merge â†’ rerank â†’ build context â†’ LLM â†’ answer
"""

from __future__ import annotations
import re
from typing import Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

from models.embedding import get_embedding
from models.law_model import vector_search, keyword_search
from services.prompt_builder import RAG_PROMPT, build_context, format_citations
from services.openrouter_service import get_llm
from services.reranker import rerank


def extract_legal_references(question: str) -> dict[str, list[str]]:
    """
    PhÃ¡t hiá»‡n cÃ¡c tham chiáº¿u ChÆ°Æ¡ng/Äiá»u cá»¥ thá»ƒ trong cÃ¢u há»i báº±ng regex.

    VÃ­ dá»¥:
        "Äiá»u 2 vÃ  Äiá»u 185 quy Ä‘á»‹nh gÃ¬?"
        â†’ {"articles": ["2", "185"], "chapters": []}

    Returns:
        Dict vá»›i keys: articles, chapters (list[str]).
    """
    # Äiá»u X
    articles = re.findall(r"[Ä‘d]i[eá»á»‡á»ƒáº¹áº»]u\s+?(\d+)", question, re.IGNORECASE)

    # ChÆ°Æ¡ng X
    chapters = re.findall(r"ch[Æ°Æ°Æ¡]ng\s+([\dIVXLivxl]+)", question, re.IGNORECASE)

    return {
        "articles": list(dict.fromkeys(articles)),   # deduplicate, giá»¯ thá»© tá»±
        "chapters": list(dict.fromkeys(chapters)),
    }


def _build_chain():
    """
    XÃ¢y dá»±ng LCEL chain cho RAG.

    Luá»“ng:
        {"context": ..., "question": ...}
        â†’ RAG_PROMPT
        â†’ ChatOpenAI (OpenRouter)
        â†’ StrOutputParser
    """
    llm = get_llm()
    chain = RAG_PROMPT | llm | StrOutputParser()
    return chain


def run_rag(
    question: str,
) -> dict[str, Any]:
    """
    Cháº¡y toÃ n bá»™ RAG pipeline tá»± Ä‘á»™ng dá»±a trÃªn ngÆ°á»¡ng Ä‘iá»ƒm sá»‘ (Threshold-based).

    Quy trÃ¬nh:
    1. Embed cÃ¢u há»i.
    2. Keyword search: phÃ¡t hiá»‡n ChÆ°Æ¡ng/Äiá»u cá»¥ thá»ƒ â†’ fetch DB (sim = 1.0).
    3. Vector search: láº¥y top 50 á»©ng viÃªn gáº§n nháº¥t.
    4. Lá»c & Merge:
       - Chá»‰ giá»¯ á»©ng viÃªn Vector cÃ³ similarity >= 0.8.
       - Gá»™p vá»›i Keyword hits (náº¿u cÃ³).
    5. Rerank: cháº¥m láº¡i toÃ n bá»™ á»©ng viÃªn, chá»‰ giá»¯ káº¿t quáº£ cÃ³ score >= 0.7.
    6. Build context â†’ Invoke LLM â†’ Tráº£ vá» káº¿t quáº£.

    Args:
        question: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

    Returns:
        {"answer": str, "citations": List[str], "chunks": List[dict], "candidates": List[dict]}
    """
    SIM_THRESHOLD = 0.7
    RERANK_THRESHOLD = 0.7
    MAX_CANDIDATES_FETCH = 50

    # 1. Embedding cÃ¢u há»i
    query_vec = get_embedding(question)

    # 2. Keyword search: phÃ¡t hiá»‡n ChÆ°Æ¡ng/Äiá»u trong cÃ¢u há»i
    refs = extract_legal_references(question)
    kw_hits = keyword_search(
        articles=refs["articles"] or None,
        chapters=refs["chapters"] or None,
    )
    if kw_hits:
        print(f"ğŸ”‘ Keyword hits: {len(kw_hits)} chunks (articles={refs['articles']}, chapters={refs['chapters']})")

    # 3. Vector search sÆ¡ bá»™ (láº¥y máº«u rá»™ng)
    vec_results = vector_search(query_vec, top_k=MAX_CANDIDATES_FETCH)

    # 4. Lá»c theo ngÆ°á»¡ng similarity 0.8 cho káº¿t quáº£ vector
    vec_filtered = [c for c in vec_results if c.get("similarity", 0) >= SIM_THRESHOLD]

    # 5. Merge + deduplicate (Keyword hits luÃ´n Ä‘Æ°á»£c giá»¯ vÃ  Ä‘á»©ng trÆ°á»›c)
    seen_ids: set = set()
    candidates: list[dict] = []
    for chunk in kw_hits + vec_filtered:
        cid = chunk.get("id")
        if cid not in seen_ids:
            seen_ids.add(cid)
            candidates.append(chunk)

    if not candidates:
        return {
            "answer":    "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u luáº­t nÃ o Ä‘á»§ Ä‘á»™ tin cáº­y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y (Similarity < 0.8).",
            "citations": [],
            "chunks":    [],
            "candidates": [],
        }

    # 6. Rerank: lá»c theo ngÆ°á»¡ng rerank_score >= 0.7
    chunks = rerank(question, candidates, score_threshold=RERANK_THRESHOLD)

    if not chunks:
        return {
            "answer":    "TÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng Ä‘á»§ cao (Rerank < 0.7) Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i.",
            "citations": [],
            "chunks":    [],
            "candidates": candidates,
        }

    # 7. Build context
    context = build_context(chunks)

    # 8. Invoke LCEL chain
    chain = _build_chain()
    answer = chain.invoke({"context": context, "question": question})

    # 9. Citations
    citations = format_citations(chunks)

    return {
        "answer":     answer,
        "citations":  citations,
        "chunks":     chunks,
        "candidates": candidates,   # danh sÃ¡ch báº£n ghi trÆ°á»›c rerank
    }
